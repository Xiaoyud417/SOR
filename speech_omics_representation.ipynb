{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64a94db-b5be-4876-905e-e0712f83ead4",
   "metadata": {},
   "source": [
    "Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfddd34-3de9-4f6c-a3d0-6b7ff0028fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sor import SOR\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40f22d-9ec7-48c6-97fd-cffde917c423",
   "metadata": {},
   "source": [
    "Before running, make sure the table files containing original frame-wise feature arrays are properly prepared and stored in the /1.FormantTable folder. Furthermore, you need make 5 new folders at the same root directory and name them 2.FormantExcel 3.DerivedMetric 4.SDRdata 5.PCAmodels 6.OmicsData respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083b1d87-681a-4ba8-a280-c788d14d960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpath='C:\\\\Users\\\\surgi\\\\source\\\\repos'\n",
    "path=cpath+'\\\\1.FormantTable'\n",
    "outpath=cpath+'\\\\6.OmicsData'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e274ee0-4812-4f3f-9b83-dcc5a78c3dba",
   "metadata": {},
   "source": [
    "The following two cells can execute normally if you have already prepared the 10000 iterations for FCR, VSA and F2R, \r\n",
    "and the data files are sored in 3.DerivedMetric folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e206f88e-7b96-49ed-b63c-01c3ca4ac20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdn=pd.read_csv(cpath+'\\\\3.DerivedMetric\\\\data10000_0.2.txt',sep='\\t',index_col=0)\n",
    "case=pdn.index.to_list()\n",
    "pdr=pd.read_excel(cpath+'\\\\0.basic\\\\pdr.xlsx',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7102146c-4e59-42fc-b848-4e79d6d5e560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [02:26<21:55, 146.22s/it]\u001b[A\n",
      " 20%|██        | 2/10 [05:14<21:15, 159.42s/it]\u001b[A\n",
      " 30%|███       | 3/10 [08:18<19:52, 170.41s/it]\u001b[A\n",
      " 40%|████      | 4/10 [11:32<17:59, 179.96s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [14:59<15:48, 189.69s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [18:36<13:15, 198.83s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [22:25<10:25, 208.60s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [26:23<07:16, 218.13s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [30:35<03:48, 228.53s/it]\u001b[A\n",
      "100%|██████████| 10/10 [34:56<00:00, 209.67s/it]\u001b[A\n",
      " 12%|█▎        | 1/8 [35:06<4:05:47, 2106.81s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [02:38<23:41, 158.00s/it]\u001b[A\n",
      " 20%|██        | 2/10 [05:30<22:13, 166.69s/it]\u001b[A\n",
      " 30%|███       | 3/10 [08:35<20:24, 174.88s/it]\u001b[A\n",
      " 40%|████      | 4/10 [11:50<18:18, 183.04s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [15:18<15:58, 191.78s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [18:55<13:21, 200.33s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [22:43<10:28, 209.58s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [26:43<07:18, 219.16s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [30:54<03:49, 229.20s/it]\u001b[A\n",
      "100%|██████████| 10/10 [35:16<00:00, 211.69s/it]\u001b[A\n",
      " 25%|██▌       | 2/8 [1:10:33<3:31:51, 2118.51s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [02:40<24:06, 160.71s/it]\u001b[A\n",
      " 20%|██        | 2/10 [05:33<22:23, 167.89s/it]\u001b[A\n",
      " 30%|███       | 3/10 [08:38<20:29, 175.71s/it]\u001b[A\n",
      " 40%|████      | 4/10 [11:53<18:20, 183.42s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [15:20<15:59, 191.84s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [18:57<13:21, 200.34s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [22:43<10:25, 208.59s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [26:37<07:13, 216.85s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [30:42<03:45, 225.52s/it]\u001b[A\n",
      "100%|██████████| 10/10 [35:03<00:00, 210.39s/it]\u001b[A\n",
      " 38%|███▊      | 3/8 [1:45:47<2:56:22, 2116.44s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [02:51<25:45, 171.77s/it]\u001b[A\n",
      " 20%|██        | 2/10 [05:50<23:28, 176.01s/it]\u001b[A\n",
      " 30%|███       | 3/10 [08:55<21:01, 180.16s/it]\u001b[A\n",
      " 40%|████      | 4/10 [12:08<18:30, 185.13s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [15:32<15:58, 191.79s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [19:07<13:18, 199.65s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [22:48<10:19, 206.61s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [26:43<07:11, 215.67s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [30:50<03:45, 225.55s/it]\u001b[A\n",
      "100%|██████████| 10/10 [35:05<00:00, 210.59s/it]\u001b[A\n",
      " 50%|█████     | 4/8 [2:21:03<2:21:04, 2116.08s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [02:38<23:43, 158.20s/it]\u001b[A\n",
      " 20%|██        | 2/10 [05:29<22:05, 165.66s/it]\u001b[A\n",
      " 30%|███       | 3/10 [08:32<20:16, 173.76s/it]\u001b[A\n",
      " 40%|████      | 4/10 [11:46<18:11, 181.91s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [15:11<15:50, 190.15s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [18:46<13:14, 198.50s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [22:33<10:23, 207.70s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [26:30<07:14, 217.04s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [30:37<03:46, 226.57s/it]\u001b[A\n",
      "100%|██████████| 10/10 [34:56<00:00, 209.65s/it]\u001b[A\n",
      " 62%|██████▎   | 5/8 [2:56:09<1:45:37, 2112.56s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [02:38<23:44, 158.27s/it]\u001b[A\n",
      " 20%|██        | 2/10 [05:28<22:03, 165.46s/it]\u001b[A\n",
      " 30%|███       | 3/10 [08:30<20:11, 173.08s/it]\u001b[A\n",
      " 40%|████      | 4/10 [11:43<18:03, 180.65s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [15:07<15:45, 189.14s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [18:42<13:11, 197.85s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [22:28<10:21, 207.02s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [26:24<07:13, 216.52s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [30:33<03:46, 226.48s/it]\u001b[A\n",
      "100%|██████████| 10/10 [34:52<00:00, 209.26s/it]\u001b[A\n",
      " 75%|███████▌  | 6/8 [3:31:11<1:10:18, 2109.07s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [02:38<23:47, 158.63s/it]\u001b[A\n",
      " 20%|██        | 2/10 [05:29<22:05, 165.68s/it]\u001b[A\n",
      " 30%|███       | 3/10 [08:31<20:12, 173.25s/it]\u001b[A\n",
      " 40%|████      | 4/10 [11:45<18:08, 181.42s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [15:09<15:48, 189.72s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [18:45<13:14, 198.56s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [22:32<10:23, 207.80s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [26:30<07:15, 217.56s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [30:39<03:47, 227.25s/it]\u001b[A\n",
      "100%|██████████| 10/10 [34:58<00:00, 209.88s/it]\u001b[A\n",
      " 88%|████████▊ | 7/8 [4:06:20<35:08, 2108.99s/it]  \n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [02:39<23:53, 159.27s/it]\u001b[A\n",
      " 20%|██        | 2/10 [05:30<22:11, 166.42s/it]\u001b[A\n",
      " 30%|███       | 3/10 [08:34<20:20, 174.35s/it]\u001b[A\n",
      " 40%|████      | 4/10 [11:47<18:11, 181.84s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [15:12<15:50, 190.07s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [18:47<13:14, 198.60s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [22:34<10:23, 207.87s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [26:32<07:15, 217.53s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [30:42<03:47, 227.49s/it]\u001b[A\n",
      "100%|██████████| 10/10 [35:02<00:00, 210.29s/it]\u001b[A\n",
      "100%|██████████| 8/8 [4:41:33<00:00, 2111.65s/it]\n"
     ]
    }
   ],
   "source": [
    "for th in tqdm(range(2, 10)):\n",
    "    threshold = th / 10\n",
    "    pdn = pd.read_csv(cpath + '\\\\3.DerivedMetric\\\\data10000_{}.txt'.format(str(threshold)),sep='\\t',index_col=0)\n",
    "\n",
    "    for thr in tqdm(range(10, 101, 10)):\n",
    "        interval = thr\n",
    "        epo = SOR.SOR(path=path, outpath=outpath, case=case, thresholds=threshold, intervals=interval, pdr=pdr)\n",
    "        pdl = epo.get_pdl(pdn, spath=cpath + '\\\\4.SDRdata')\n",
    "        npd0, pddata = epo.get_ALL(pdl, spath=cpath + '\\\\4.SDRdata')\n",
    "        epo.PCA(npd0, pddata, ppath=cpath + '\\\\5.PCAmodels')\n",
    "    time.sleep\n",
    "\n",
    "ofile=[]\n",
    "for file in os.listdir(cpath + '\\\\6.OmicsData'):\n",
    "    of0=pd.read_csv(cpath + '\\\\6.OmicsData\\\\'+file,sep='\\t',index_col=0)\n",
    "    ofile.append(of0)\n",
    "pan_file=pd.concat(ofile,axis=1)\n",
    "pan_file.to_csv(cpath+'\\\\Speech_Omics_data.txt',sep='\\t',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc92845-9950-44f8-b265-5177a3fe9808",
   "metadata": {},
   "source": [
    "If you need to restart 10000 iterations of computing FCR, VSA and F2R, please run the following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85735f-12fe-4c9b-ab93-c5fea27d4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = list(set([fn[4:21] for fn in os.listdir(path)]))\n",
    "pdr = pd.read_excel(cpath + '\\\\0.basic\\\\pdr.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb9582-a847-455d-b98d-f8f08f298ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epo = SOR.SOR(path=path, outpath=outpath, case=case, thresholds=1, intervals=1, pdr=pdr)\n",
    "pkde = epo.get_kde(kpath=cpath + '\\\\2.FormantExcel')\n",
    "for th in tqdm(range(2, 10)):\n",
    "    threshold = th / 10\n",
    "    epo = SOR.SOR(path=path, outpath=outpath, case=case, thresholds=threshold, intervals=1, pdr=pdr)\n",
    "    pdn = epo.get_pdn(ipath=cpath + '\\\\3.DerivedMetric') # This step is time-consuming.\n",
    "\n",
    "    for thr in tqdm(range(10, 101, 10)):\n",
    "        interval = thr\n",
    "        epo = SOR.SOR(path=path, outpath=outpath, case=case, thresholds=threshold, intervals=interval, pdr=pdr)\n",
    "        pdl = epo.get_pdl(pdn, spath=cpath + '\\\\4.SDRdata')\n",
    "        npd0, pddata = epo.get_ALL(pdl, spath=cpath + '\\\\4.SDRdata')\n",
    "        epo.PCA(npd0, pddata, ppath=cpath + '\\\\5.PCAmodels')\n",
    "    time.sleep\n",
    "\n",
    "ofile=[]\n",
    "for file in os.listdir(cpath + '\\\\6.OmicsData'):\n",
    "    of0=pd.read_csv(cpath + '\\\\6.OmicsData\\\\'+file,sep='\\t',index_col=0)\n",
    "    ofile.append(of0)\n",
    "pan_file=pd.concat(ofile,axis=1)\n",
    "pan_file.to_csv(cpath+'\\\\Speech_Omics_data.txt',sep='\\t',encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
